from services import knowledge_base


def get_loading_block(status_text: str) -> list:
    """Creates a lightweight loading indicator to give a pseudo-animation effect."""
    return [
        {
            "type": "context",
            "elements": [
                {"type": "mrkdwn", "text": f":gear: *Evidently is thinking...* | {status_text}"}
            ],
        },
        {
            "type": "section",
            "text": {"type": "mrkdwn", "text": "Please wait while I consult the playbook..."},
        },
    ]


def _generate_confidence_meter(score: int) -> str:
    """Creates a visual progress bar using emoji blocks to show confidence."""
    score = max(0, min(100, score or 0))
    filled_symbol = "ðŸŸ©" if score > 70 else "ðŸŸ¨" if score > 40 else "ðŸŸ¥"
    empty_symbol = "â¬œ"
    filled_blocks = round(score / 10)
    return f"`{filled_symbol * filled_blocks}{empty_symbol * (10 - filled_blocks)}` {score}%"


def get_ai_summary_block(analysis: dict) -> list:
    """Renders the AI analysis with visual confidence meters and call-to-action buttons."""
    summary = analysis.get("summary") or analysis.get("so_what_summary") or "Analysis complete."
    blocks: list = [
        {"type": "header", "text": {"type": "plain_text", "text": "âœ¨ Insight Analysis"}},
        {
            "type": "section",
            "text": {"type": "mrkdwn", "text": f"*{summary}*"},
        },
        {"type": "divider"},
    ]

    decisions = analysis.get("decisions") or []
    if decisions:
        blocks.append(
            {
                "type": "section",
                "text": {"type": "mrkdwn", "text": "*Hard Decisions*\n- " + "\n- ".join(decisions)},
            }
        )
        blocks.append({"type": "divider"})

    assumptions = analysis.get("assumptions") or []
    if assumptions:
        blocks.append({"type": "section", "text": {"type": "mrkdwn", "text": "*Extracted Assumptions (OCP)*"}})
        for item in assumptions:
            meter = _generate_confidence_meter(item.get("confidence", item.get("confidence_score", 50)))
            category = item.get("category", "Unknown")
            category_icon = {"Opportunity": "ðŸŸ¡", "Capability": "ðŸŸ¢", "Progress": "ðŸ”µ"}.get(category, "âšª")
            blocks.append(
                {
                    "type": "section",
                    "text": {
                        "type": "mrkdwn",
                        "text": f"{category_icon} *{category}*: {item.get('text', 'Assumption pending')}\n{meter}",
                    },
                    "accessory": {
                        "type": "button",
                        "text": {"type": "plain_text", "text": "Design Test"},
                        "value": f"test_{item.get('text', '')[:20]}",
                        "action_id": "gen_experiment_modal",
                    },
                }
            )

    blocks.append(
        {
            "type": "context",
            "elements": [
                {"type": "mrkdwn", "text": "Generated by Evidently Â· Google Gemini"},
                {"type": "mrkdwn", "text": "ðŸ›¡ï¸ Processed securely with PII redaction active."},
            ],
        }
    )
    return blocks


def get_decision_room_blocks(session_id: str, status: str, results: dict | None = None) -> list:
    """Builds dynamic blocks for the Decision Room session."""
    header = {
        "type": "header",
        "text": {"type": "plain_text", "text": "ðŸ—³ï¸ Decision Room"},
    }

    if status == "waiting":
        return [
            header,
            {
                "type": "section",
                "text": {
                    "type": "mrkdwn",
                    "text": "Invite the team to score impact and uncertainty. Votes stay hidden until everyone responds.",
                },
            },
            {
                "type": "actions",
                "elements": [
                    {
                        "type": "button",
                        "text": {"type": "plain_text", "text": "Reveal Votes"},
                        "action_id": "reveal_decision_votes",
                        "value": session_id,
                    }
                ],
            },
        ]

    results = results or {"avg_impact": 0, "avg_uncertainty": 0, "count": 0}
    score_meter = _generate_confidence_meter(max(0, 100 - int(results.get("avg_uncertainty", 0) * 10)))
    revealed_blocks = [
        header,
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": (
                    f"*Votes revealed*\nParticipants: {results.get('count', 0)}\n"
                    f"Average impact: {results.get('avg_impact', 0)}\n"
                    f"Average uncertainty: {results.get('avg_uncertainty', 0)}\n"
                    f"Confidence trend: {score_meter}"
                ),
            },
        },
    ]

    heatmap_url = results.get("heatmap_url")
    if heatmap_url:
        revealed_blocks.append({"type": "image", "image_url": heatmap_url, "alt_text": "Consensus heatmap"})
    return revealed_blocks


def get_nudge_block(assumption: dict) -> list:
    if not all(k in assumption for k in ("id", "text")):
        raise ValueError("Assumption dictionary must contain 'id' and 'text' keys.")
    return [
        {
            "type": "section",
            "text": {
                "type": "mrkdwn",
                "text": (
                    ":alarm_clock: *Stale Assumption Alert*\n\n"
                    f"This assumption hasn't been tested in 2 weeks: *{assumption['text']}*"
                ),
            },
        },
        {
            "type": "actions",
            "elements": [
                {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "Keep Testing"},
                    "style": "primary",
                    "action_id": "keep_testing",
                    "value": str(assumption["id"]),
                },
                {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "Archive"},
                    "style": "danger",
                    "action_id": "archive_assumption",
                    "value": str(assumption["id"]),
                },
                {
                    "type": "button",
                    "text": {"type": "plain_text", "text": "Validate"},
                    "action_id": "validate_assumption",
                    "value": str(assumption["id"]),
                },
            ],
        },
    ]


def decision_vote_modal(assumption_title: str, assumption_id: int, channel_id: str) -> dict:
    return {
        "type": "modal",
        "callback_id": "submit_decision_vote",
        "private_metadata": f"{assumption_id}:{channel_id}",
        "title": {"type": "plain_text", "text": "Decision Vote"},
        "submit": {"type": "plain_text", "text": "Submit"},
        "close": {"type": "plain_text", "text": "Cancel"},
        "blocks": [
            {
                "type": "section",
                "text": {"type": "mrkdwn", "text": f"Scoring Assumption:\n*{assumption_title}*"},
            },
            {"type": "divider"},
            {
                "type": "input",
                "block_id": "impact_block",
                "label": {"type": "plain_text", "text": "Impact (1 = low, 5 = high)"},
                "element": {
                    "type": "static_select",
                    "action_id": "impact_score",
                    "options": [
                        {"text": {"type": "plain_text", "text": str(i)}, "value": str(i)}
                        for i in range(1, 6)
                    ],
                },
            },
            {
                "type": "input",
                "block_id": "uncertainty_block",
                "label": {"type": "plain_text", "text": "Uncertainty (1 = low, 5 = high)"},
                "element": {
                    "type": "static_select",
                    "action_id": "uncertainty_score",
                    "options": [
                        {"text": {"type": "plain_text", "text": str(i)}, "value": str(i)}
                        for i in range(1, 6)
                    ],
                },
            },
        ],
    }


def decision_heatmap_label(avg_impact: float, avg_uncertainty: float) -> str:
    if avg_impact >= 4 and avg_uncertainty >= 4:
        return "High Impact / High Uncertainty â†’ Do this first"
    if avg_impact >= 4 and avg_uncertainty < 4:
        return "High Impact / Lower Uncertainty â†’ Move quickly"
    if avg_impact < 4 and avg_uncertainty >= 4:
        return "Lower Impact / High Uncertainty â†’ Park and learn"
    return "Lower Impact / Lower Uncertainty â†’ Schedule later"


def error_block(message: str) -> list:
    """User-friendly error banner."""
    return [
        {
            "type": "section",
            "text": {"type": "mrkdwn", "text": f"ðŸš§ *Something went wrong.* {message}"},
        }
    ]


def method_card(method: str) -> dict:
    description = knowledge_base.get_case_study(method) or "Nesta Playbook method."
    return {
        "type": "section",
        "text": {"type": "mrkdwn", "text": f"*{method}*\n{description}"},
        "accessory": {
            "type": "button",
            "text": {"type": "plain_text", "text": "View Case Study"},
            "action_id": "view_case_study",
            "value": method,
        },
    }


def case_study_modal(method: str) -> dict:
    description = knowledge_base.get_case_study(method) or "Case study coming soon."
    return {
        "type": "modal",
        "callback_id": "case_study_modal",
        "title": {"type": "plain_text", "text": "Case Study"},
        "close": {"type": "plain_text", "text": "Close"},
        "blocks": [
            {"type": "header", "text": {"type": "plain_text", "text": method}},
            {"type": "section", "text": {"type": "mrkdwn", "text": description}},
        ],
    }
